{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Machine Learning Basics\n",
    "\n",
    "## Aneesh Gupta\n",
    "NetId:  ag468\n",
    "\n",
    "**Instructions**: You may complete this in whatever environment you prefer, however, the document you turn in should be neatly and coherently prepared. This could be done in a Jupyter Notebook, or in a Latex report, Word Document, etc. Part of your score will be devoted to the quality and professionalism of the presentation of your work. This is an important skill to master for being able to clearly communicate about your work. What that means:\n",
    "- Are your explanations clear and coherent?\n",
    "- Are all plots clear (not blurry), and all text on plots readable (no too small). Are all axes labeled? Is it clear what each plot represents from either a figure caption (text after the plot) or a title? Are legends present if there are multiple types of objects on a single plot?\n",
    "- Are all answers clearly indicated? Is your document easy to follow?\n",
    "- Is code well-documented and follows consistent programming practices (such as PEP 8)?\n",
    "- If I were an employer considering highering you - would this document make me more likely to want to higher you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "1. Be able to apply basic classification supervised learning techniques to data and evaluate the performance of those methods\n",
    "2. Understand the bias-variance tradeoff and how adjusting model flexibility impacts model selection and the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "**[5 points]**\n",
    "For each part (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n",
    "\n",
    "1. The sample size $n$ is extremely large, and the number of predictors $p$ is small.\n",
    "2. The number of predictors $p$ is extremely large, and the number of observations $n$ is small.\n",
    "3. The relationship between the predictors and response is highly non-linear.\n",
    "4. The variance of the error terms, i.e. $\\sigma^2 = Var(\\epsilon)$, is extremely high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "1. A _flexible_ learning model will be better suited, as with a large sample size (n). A higher flexibility will allow the model to fit to the existing data points better. A flexible data can have higher variance and fit to more and more data points. Since the data set is so large, the 'risk' of overfitting is somewhat diminshed than for a smaller dataset.\n",
    "\n",
    "2. With a smaller set of data, generalization becomes more difficult. Here, we would like to avoid overfitting, and therefore a _inflexible_ learning model would be better suited, as it would not overfit.\n",
    "\n",
    "3. With a highly non-linear relationship, we would prefer a _flexible_ method, because it would allow our model to adjust to this non-linearity better than an inflexible method.\n",
    "\n",
    "4. Because the error terms have a high variance, I would stick to an _inflexible_ model so that I don't accidentally over fit to the erroneous data points, as _flexible_ models can often lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "**[5 points]** For each of the following, (i) explain if each scenario is a classification or regression problem, (ii) indicate whether we are most interested in inference or prediction for that problem, and (iii) provide the sample size $n$ and number of predictors $p$ indicated for each scenario.\n",
    "\n",
    "**(a)** We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n",
    "\n",
    "**(b)** We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n",
    "\n",
    "**(c)** We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "**(a)** This is a regression problem, as we want to draw a relation between two variables. We are most interested in inference (again, relation and not the final outcome is desired). The sample size $n$ is 500, we have 3 different predictors (profit, # of employees, industry) and the 'response' is CEO Salary.\n",
    "\n",
    "**(b)** This is a classification problem, we want to classify the new product as a success or as a failure. We are interested in prediction, predicting the success outcome.The sample size $n$ is 20, the number of predictors are 13, and the 14th variable, success is the response.\n",
    "\n",
    "**(c)** This is a regression problem, as we want the relation between the USD and global stock markets. We are intersted in prediction (predicting the percent change). The sample size, $n$, is 52 (weeks) and the number of predictors is 3 (British, US, and German Markets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "**[20 points] Classification I: Creating a classification algorithm**.\n",
    "\n",
    "**(a)** Build a working version of a binary kNN classifier using the skeleton code below.\n",
    "\n",
    "**(b)** Load the datasets to be evaluated here. Each includes training features ($\\mathbf{X}$), and test features ($\\mathbf{y}$) for both a low dimensional ($p = 2$ features/predictors) and a high dimensional ($p = 100$ features/predictors). For each of these datasets there are $n=100$ observations of each. They can be found in the `data` subfolder in the `assignments` folder on github. Each file is labeled similar to `A1_X_train_low.csv`, which lets you know whether the dataset is of features, $X$, targets, $y$; training or testing; and low or high dimensions.\n",
    "\n",
    "**(c)** Train your classifier on first the low dimensional dataset and then the high dimensional dataset with $k=5$. Evaluate the classification performance on the corresponding test data for each. Calculate the time it takes to make the predictions in each case and the overall accuracy of each set of test data predictions.\n",
    "\n",
    "**(d)** Compare your implementation's accuracy and computation time to the scikit learn [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class. How do the results and speed compare? *Note: the high and low dimensional problems are different, so performance is not comparable in terms of accuracy across datasets, but IS comparable across algorithms. In other words - how efficient and accurate was your algorithm vs the scikit-learn implementation?*\n",
    "\n",
    "**(e)** Some supervised learning algorithms are more computationally intensive during training than testing. What are the drawbacks of the prediction process being slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (a) Write your own kNN classifier\n",
    "\n",
    "class Knn:\n",
    "# k-Nearest Neighbor class object for classification training and testing\n",
    "    def __init__(self):\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        # Save the training data to properties of this class\n",
    "        \n",
    "    def predict(self, x, k):\n",
    "        y_hat = [] # Variable to store the estimated class label for \n",
    "        # Calculate the distance from each vector in x to the training data\n",
    "        \n",
    "        # Return the estimated targets\n",
    "        return y_hat\n",
    "\n",
    "# Metric of overall classification accuracy\n",
    "#  (a more general function, sklearn.metrics.accuracy_score, is also available)\n",
    "def accuracy(y,y_hat):\n",
    "    nvalues = len(y)\n",
    "    accuracy = sum(y == y_hat) / nvalues\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of your kNN classifier on a low- and a high-dimensional dataset \n",
    "#   and time the predictions of each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "**[20 points] Bias-variance tradeoff I: Understanding the tradeoff**. This exercise will illustrate the impact of the bias-variance tradeoff on classifier performance by looking at classifier decision boundaries.\n",
    "\n",
    "**(a)** Create a synthetic dataset (with both features and targets). Use the [`make_moons`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons) module with the parameter `noise=0.35` to generate 1000 random samples.\n",
    "\n",
    "**(b)** Scatterplot your random samples with each class in a different color\n",
    "\n",
    "**(c)** Create 3 different data subsets by selecting 150 of the 1000 data points at random three times. For each of these 150-sample datasets, fit three k-Nearest Neighbor classifier with: $k = \\{1, 25, 100\\}$. This will result in 9 combinations (3 datasets, with 3 trained classifiers).\n",
    "\n",
    "For each combination of dataset trained classifier, in a 3-by-3 grid, plot the decision boundary (similar in style to Figure 2.15 from *Introduction to Statistical Learning*). Each column should represent a different value of $k$ and each row should represent a different dataset. \n",
    "\n",
    "As you're doing this, also create a larger (but separate) test dataset using the same approach as in **(a)**, but this time with 50,000 samples. Evaluate the classification accuracy for each of the 9 decision boundaries and include the accuracy in the title. Here we define accuracy as the percent of samples correctly classified.\n",
    "\n",
    "**(e)** What do you notice about the difference between the rows and the columns. Which decision boundaries appear to best separate the two classes of data? Which decision boundaries vary the most as the data change?\n",
    "\n",
    "**(f)** Explain the bias-variance tradeoff using the example of the plots you made in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
